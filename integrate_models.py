#!/usr/bin/env python3
"""
Model Integration Script
Automatically configures downloaded models for use with the platform
"""

import os
import sys
from pathlib import Path

try:
    from models_config import MODELS_CONFIG, MODELS_CONFIG_LITE
except ImportError:
    print("Error: models_config.py not found.")
    sys.exit(1)


def check_models_exist(use_lite=False):
    """Check which models are available."""
    models_dir = Path('./models')
    if not models_dir.exists():
        return {}
    
    models_config = MODELS_CONFIG_LITE if use_lite else MODELS_CONFIG
    available_models = {}
    
    for model_key, model_info in models_config.items():
        model_path = models_dir / model_info['filename']
        if model_path.exists() and model_path.stat().st_size > 0:
            available_models[model_key] = str(model_path)
    
    return available_models


def generate_model_service_integration(available_models):
    """Generate integration code for model_service.py"""
    
    integration_code = '''"""
AUTO-GENERATED MODEL INTEGRATION
This section is automatically generated by integrate_models.py
To use actual models, uncomment and configure the sections below
"""

# Uncomment to use actual llama-cpp-python for GGUF models
# You'll need to install: pip install llama-cpp-python
# For GPU support: CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python

'''
    
    if available_models:
        integration_code += "# Available models found:\n"
        for model_key, model_path in available_models.items():
            integration_code += f"# - {model_key}: {model_path}\n"
        integration_code += "\n"
        
        integration_code += '''
# Example integration for LlamaCppAdapter:
# from llama_cpp import Llama
# 
# class LlamaCppAdapter(ModelAdapter):
#     def __init__(self, model_path="./models/llama-2-7b.Q4_K_M.gguf"):
#         try:
#             self.model = Llama(
#                 model_path=model_path,
#                 n_ctx=4096,  # Context window
#                 n_threads=4,  # CPU threads
#                 n_gpu_layers=0,  # Set to -1 for full GPU offload
#             )
#             self.is_mock = False
#         except Exception as e:
#             print(f"Warning: Could not load model {model_path}: {e}")
#             self.model = None
#             self.is_mock = True
#     
#     def generate(self, prompt, user=None):
#         if self.is_mock or self.model is None:
#             return "[Mock response - model not loaded]"
#         
#         response = self.model(
#             prompt,
#             max_tokens=512,
#             temperature=0.7,
#             top_p=0.95,
#             stop=["User:", "Assistant:"],
#         )
#         return response['choices'][0]['text']

# Similar integration can be done for DeepSeekAdapter and GPT4AllAdapter
'''
    
    return integration_code


def create_env_config(available_models):
    """Create environment configuration for models."""
    env_lines = [
        "\n# Model Configuration (Auto-generated)",
        "# Paths to downloaded models",
    ]
    
    for model_key, model_path in available_models.items():
        env_key = f"MODEL_{model_key.upper().replace('-', '_')}_PATH"
        env_lines.append(f"{env_key}={model_path}")
    
    return "\n".join(env_lines)


def main():
    """Main integration workflow."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Integrate downloaded models with the platform',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
This script checks for downloaded models and provides integration guidance.

Steps:
1. Run download_models.py to download models
2. Run this script to check integration status
3. Follow the instructions to enable actual model usage

For automatic usage, models work in mock mode by default.
To use real models, you need to:
- Install llama-cpp-python: pip install llama-cpp-python
- Uncomment integration code in app/services/model_service.py
- Restart the application
        """
    )
    
    parser.add_argument(
        '--lite',
        action='store_true',
        help='Check for lite model variants'
    )
    
    parser.add_argument(
        '--generate-env',
        action='store_true',
        help='Generate environment configuration for .env file'
    )
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("üîß Model Integration Status")
    print("=" * 70)
    print()
    
    # Check for available models
    available_models = check_models_exist(args.lite)
    
    if not available_models:
        print("‚ùå No models found in ./models directory")
        print("\nüì• Run this command to download models:")
        print("   python download_models.py")
        if args.lite:
            print("   python download_models.py --lite")
        print()
        return
    
    print(f"‚úÖ Found {len(available_models)} model(s):")
    for model_key, model_path in available_models.items():
        file_size = Path(model_path).stat().st_size / (1024**3)  # GB
        print(f"   ‚Ä¢ {model_key}: {model_path} ({file_size:.2f} GB)")
    print()
    
    # Current status
    print("üìä Current Status:")
    print("   ‚Ä¢ Models are downloaded and ready")
    print("   ‚Ä¢ Platform is using MOCK adapters (placeholder responses)")
    print("   ‚Ä¢ To use real models, follow integration steps below")
    print()
    
    # Integration steps
    print("üîß Integration Steps:")
    print()
    print("1. Install llama-cpp-python:")
    print("   pip install llama-cpp-python")
    print()
    print("   For GPU acceleration (NVIDIA CUDA):")
    print("   CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python")
    print()
    print("   For Apple Silicon (M1/M2/M3/M4) - Metal acceleration:")
    print("   CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python")
    print()
    
    print("2. Update app/services/model_service.py:")
    print("   See integration example in the auto-generated comment section")
    print()
    
    print("3. Restart the application:")
    print("   docker-compose restart web")
    print("   # or")
    print("   python run.py")
    print()
    
    # Generate environment config
    if args.generate_env:
        env_config = create_env_config(available_models)
        print("üìù Environment Configuration:")
        print(env_config)
        print()
        print("Add the above to your .env file")
        print()
    
    # Show integration code example
    print("üí° Integration Code Example:")
    print("-" * 70)
    integration_code = generate_model_service_integration(available_models)
    print(integration_code)
    print("-" * 70)
    print()
    
    print("‚úÖ Integration check complete!")
    print()
    print("üìö For more details, see docs/MODELS.md")


if __name__ == '__main__':
    main()
