#!/usr/bin/env python3
"""
Model Integration Script
Automatically configures downloaded models for use with the platform
"""

import os
import sys
import subprocess
from pathlib import Path

try:
    from models_config import MODELS_CONFIG, MODELS_CONFIG_LITE
except ImportError:
    print("Error: models_config.py not found.")
    sys.exit(1)


def check_models_exist(use_lite=False):
    """Check which models are available."""
    models_dir = Path('./models')
    if not models_dir.exists():
        return {}
    
    models_config = MODELS_CONFIG_LITE if use_lite else MODELS_CONFIG
    available_models = {}
    
    for model_key, model_info in models_config.items():
        model_path = models_dir / model_info['filename']
        if model_path.exists() and model_path.stat().st_size > 0:
            available_models[model_key] = str(model_path)
    
    return available_models


def generate_model_service_integration(available_models):
    """Generate integration code for model_service.py"""
    
    integration_code = '''"""
AUTO-GENERATED MODEL INTEGRATION
This section is automatically generated by integrate_models.py
To use actual models, uncomment and configure the sections below
"""

# Uncomment to use actual llama-cpp-python for GGUF models
# You'll need to install: pip install llama-cpp-python
# For GPU support: CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python

'''
    
    if available_models:
        integration_code += "# Available models found:\n"
        for model_key, model_path in available_models.items():
            integration_code += f"# - {model_key}: {model_path}\n"
        integration_code += "\n"
        
        integration_code += '''
# Example integration for LlamaCppAdapter:
# from llama_cpp import Llama
# 
# class LlamaCppAdapter(ModelAdapter):
#     def __init__(self, model_path="./models/llama-2-7b.Q4_K_M.gguf"):
#         try:
#             self.model = Llama(
#                 model_path=model_path,
#                 n_ctx=4096,  # Context window
#                 n_threads=4,  # CPU threads
#                 n_gpu_layers=0,  # Set to -1 for full GPU offload
#             )
#             self.is_mock = False
#         except Exception as e:
#             print(f"Warning: Could not load model {model_path}: {e}")
#             self.model = None
#             self.is_mock = True
#     
#     def generate(self, prompt, user=None):
#         if self.is_mock or self.model is None:
#             return "[Mock response - model not loaded]"
#         
#         response = self.model(
#             prompt,
#             max_tokens=512,
#             temperature=0.7,
#             top_p=0.95,
#             stop=["User:", "Assistant:"],
#         )
#         return response['choices'][0]['text']

# Similar integration can be done for DeepSeekAdapter and GPT4AllAdapter
'''
    
    return integration_code


def create_env_config(available_models):
    """Create environment configuration for models."""
    env_lines = [
        "\n# Model Configuration (Auto-generated)",
        "# Paths to downloaded models",
    ]
    
    for model_key, model_path in available_models.items():
        env_key = f"MODEL_{model_key.upper().replace('-', '_')}_PATH"
        env_lines.append(f"{env_key}={model_path}")
    
    return "\n".join(env_lines)


def install_llama_cpp_python():
    """Install llama-cpp-python with appropriate flags."""
    print("\nüì¶ Installing llama-cpp-python...")
    print("   This may take a few minutes...")
    
    # Detect platform
    import platform
    system = platform.system()
    machine = platform.machine()
    
    try:
        # Check if already installed
        import llama_cpp
        print("   ‚úÖ llama-cpp-python is already installed")
        return True
    except ImportError:
        pass
    
    # Determine installation command
    if system == "Darwin" and machine == "arm64":
        # Apple Silicon - use Metal acceleration
        print("   Detected Apple Silicon - installing with Metal acceleration...")
        cmd = ['pip', 'install', 'llama-cpp-python', '--no-cache-dir']
        env = os.environ.copy()
        env['CMAKE_ARGS'] = '-DLLAMA_METAL=on'
    elif system == "Linux":
        # Linux - try CUDA if available, otherwise CPU
        print("   Detected Linux - installing with CPU support...")
        print("   (For GPU support, manually install with CUDA flags)")
        cmd = ['pip', 'install', 'llama-cpp-python', '--no-cache-dir']
        env = os.environ.copy()
    else:
        # Windows or other - CPU only
        print("   Installing with CPU support...")
        cmd = ['pip', 'install', 'llama-cpp-python', '--no-cache-dir']
        env = os.environ.copy()
    
    try:
        result = subprocess.run(cmd, env=env, capture_output=True, text=True, timeout=600)
        if result.returncode == 0:
            print("   ‚úÖ llama-cpp-python installed successfully!")
            return True
        else:
            print(f"   ‚ùå Installation failed: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("   ‚ùå Installation timed out (>10 minutes) - compilation may need more time")
        return False
    except Exception as e:
        print(f"   ‚ùå Installation error: {str(e)}")
        return False


def update_model_service(available_models):
    """Update model_service.py to use actual models."""
    service_file = Path('./app/services/model_service.py')
    
    if not service_file.exists():
        print("   ‚ùå model_service.py not found")
        return False
    
    print("\nüìù Updating model_service.py...")
    
    # Read current content
    with open(service_file, 'r') as f:
        content = f.read()
    
    # Check if already integrated
    if 'AUTO_INTEGRATED' in content:
        print("   ‚ÑπÔ∏è  Models already integrated in model_service.py")
        return True
    
    # Create updated model service with actual model loading
    updated_service = '''"""Model service for AI interactions."""
from abc import ABC, abstractmethod
import random
import re
from pathlib import Path

# AUTO_INTEGRATED: This file has been automatically integrated with downloaded models
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    print("Warning: llama-cpp-python not available, using mock adapters")


class ModelAdapter(ABC):
    """Base class for model adapters."""
    
    @abstractmethod
    def generate(self, prompt, user=None):
        """Generate a response from the model."""
        pass
    
    @abstractmethod
    def get_name(self):
        """Get model name."""
        pass
    
    @abstractmethod
    def is_loaded(self):
        """Check if model is loaded."""
        pass


class LlamaCppAdapter(ModelAdapter):
    """Adapter for llama.cpp models."""
    
    def __init__(self, model_path=None):
        self.model_path = model_path or './models/llama-2-7b.Q4_K_M.gguf'
        self.model = None
        self._is_loaded = False
        
        if LLAMA_CPP_AVAILABLE and Path(self.model_path).exists():
            try:
                print(f"Loading Llama model from {self.model_path}...")
                self.model = Llama(
                    model_path=self.model_path,
                    n_ctx=4096,
                    n_threads=4,
                    n_gpu_layers=0,  # Adjust for GPU
                    verbose=False
                )
                self._is_loaded = True
                print(f"‚úÖ Llama model loaded successfully")
            except Exception as e:
                print(f"Warning: Could not load Llama model: {e}")
                self._is_loaded = False
    
    def is_loaded(self):
        return self._is_loaded
    
    def generate(self, prompt, user=None):
        """Generate response using llama.cpp."""
        if self._is_loaded and self.model:
            try:
                response = self.model(
                    prompt,
                    max_tokens=512,
                    temperature=0.7,
                    top_p=0.95,
                    stop=["User:", "\\n\\nUser:", "\\n\\nQuestion:"],
                    echo=False
                )
                return response['choices'][0]['text'].strip()
            except Exception as e:
                print(f"Error generating response: {e}")
                return self._mock_response(prompt)
        else:
            return self._mock_response(prompt)
    
    def _mock_response(self, prompt):
        """Fallback mock response."""
        file_responses = [
            f"[Llama.cpp - Document Processing]\\n\\nAnalyzing document content: {prompt[:80]}...\\n\\nKey information extracted:\\n- Document structure analysis\\n- Content summarization\\n- Key points identification\\n\\nNote: This is a mock response. Real model not loaded.",
        ]
        return random.choice(file_responses)
    
    def get_name(self):
        return "llama.cpp"


class GPT4AllAdapter(ModelAdapter):
    """Adapter for GPT4All models."""
    
    def __init__(self, model_path=None):
        self.model_path = model_path or './models/gpt4all-falcon-newbpe-q4_0.gguf'
        self.model = None
        self._is_loaded = False
        
        if LLAMA_CPP_AVAILABLE and Path(self.model_path).exists():
            try:
                print(f"Loading GPT4All model from {self.model_path}...")
                self.model = Llama(
                    model_path=self.model_path,
                    n_ctx=2048,
                    n_threads=4,
                    n_gpu_layers=0,
                    verbose=False
                )
                self._is_loaded = True
                print(f"‚úÖ GPT4All model loaded successfully")
            except Exception as e:
                print(f"Warning: Could not load GPT4All model: {e}")
                self._is_loaded = False
    
    def is_loaded(self):
        return self._is_loaded
    
    def generate(self, prompt, user=None):
        """Generate response using GPT4All."""
        if self._is_loaded and self.model:
            try:
                response = self.model(
                    prompt,
                    max_tokens=512,
                    temperature=0.7,
                    top_p=0.95,
                    stop=["User:", "\\n\\nUser:"],
                    echo=False
                )
                return response['choices'][0]['text'].strip()
            except Exception as e:
                print(f"Error generating response: {e}")
                return self._mock_response(prompt)
        else:
            return self._mock_response(prompt)
    
    def _mock_response(self, prompt):
        """Fallback mock response."""
        responses = [
            f"Based on your query: {prompt[:50]}...\\n\\nHere's what I can tell you:\\n\\n[Mock response - model not loaded]",
        ]
        return random.choice(responses)
    
    def get_name(self):
        return "gpt4all"


class DeepSeekAdapter(ModelAdapter):
    """Adapter for DeepSeek models."""
    
    def __init__(self, model_path=None):
        self.model_path = model_path or './models/deepseek-coder-6.7b-instruct.Q4_K_M.gguf'
        self.model = None
        self._is_loaded = False
        
        if LLAMA_CPP_AVAILABLE and Path(self.model_path).exists():
            try:
                print(f"Loading DeepSeek model from {self.model_path}...")
                self.model = Llama(
                    model_path=self.model_path,
                    n_ctx=4096,
                    n_threads=4,
                    n_gpu_layers=0,
                    verbose=False
                )
                self._is_loaded = True
                print(f"‚úÖ DeepSeek model loaded successfully")
            except Exception as e:
                print(f"Warning: Could not load DeepSeek model: {e}")
                self._is_loaded = False
    
    def is_loaded(self):
        return self._is_loaded
    
    def generate(self, prompt, user=None):
        """Generate response using DeepSeek."""
        if self._is_loaded and self.model:
            try:
                # DeepSeek uses a specific prompt format for coding
                formatted_prompt = f"### Instruction:\\n{prompt}\\n\\n### Response:\\n"
                response = self.model(
                    formatted_prompt,
                    max_tokens=1024,
                    temperature=0.2,  # Lower temperature for code
                    top_p=0.95,
                    stop=["###", "\\n\\n\\n"],
                    echo=False
                )
                return response['choices'][0]['text'].strip()
            except Exception as e:
                print(f"Error generating response: {e}")
                return self._mock_response(prompt)
        else:
            return self._mock_response(prompt)
    
    def _mock_response(self, prompt):
        """Fallback mock response."""
        coding_responses = [
            f"[DeepSeek Coder]\\n\\nAnalyzing your code request: {prompt[:80]}...\\n\\n```python\\n# Here's a solution approach:\\ndef example_function():\\n    # Implementation details\\n    pass\\n```\\n\\nNote: This is a mock response. Real model not loaded.",
        ]
        return random.choice(coding_responses)
    
    def get_name(self):
        return "deepseek"


class VicunaAdapter(ModelAdapter):
    """Adapter for Vicuna models."""
    
    def __init__(self, model_path=None):
        self.model_path = model_path or './models/vicuna-7b-v1.5.Q4_K_M.gguf'
        self.model = None
        self._is_loaded = False
        
        if LLAMA_CPP_AVAILABLE and Path(self.model_path).exists():
            try:
                print(f"Loading Vicuna model from {self.model_path}...")
                self.model = Llama(
                    model_path=self.model_path,
                    n_ctx=2048,
                    n_threads=4,
                    n_gpu_layers=0,
                    verbose=False
                )
                self._is_loaded = True
                print(f"‚úÖ Vicuna model loaded successfully")
            except Exception as e:
                print(f"Warning: Could not load Vicuna model: {e}")
                self._is_loaded = False
    
    def is_loaded(self):
        return self._is_loaded
    
    def generate(self, prompt, user=None):
        """Generate response using Vicuna."""
        if self._is_loaded and self.model:
            try:
                response = self.model(
                    prompt,
                    max_tokens=512,
                    temperature=0.7,
                    top_p=0.95,
                    stop=["USER:", "ASSISTANT:"],
                    echo=False
                )
                return response['choices'][0]['text'].strip()
            except Exception as e:
                print(f"Error generating response: {e}")
                return self._mock_response(prompt)
        else:
            return self._mock_response(prompt)
    
    def _mock_response(self, prompt):
        """Fallback mock response."""
        multimodal_responses = [
            f"[Vicuna - Multimodal Analysis]\\n\\nAnalyzing your content request: {prompt[:80]}...\\n\\nNote: This is a mock response. Real model not loaded.",
        ]
        return random.choice(multimodal_responses)
    
    def get_name(self):
        return "vicuna"


# Initialize models - will auto-load if available
MODELS = {
    'llama': LlamaCppAdapter(),
    'gpt4all': GPT4AllAdapter(),
    'deepseek': DeepSeekAdapter(),
    'vicuna': VicunaAdapter()
}


def detect_content_type(prompt):
    """Detect content type from prompt to select appropriate model.
    
    Returns:
        str: Content type - 'code', 'file', 'pdf', 'image', 'video', 'general'
    """
    prompt_lower = prompt.lower()
    
    # Coding keywords
    coding_keywords = [
        'code', 'function', 'class', 'programming', 'debug', 'error',
        'python', 'javascript', 'java', 'c++', 'rust', 'go', 'php',
        'html', 'css', 'sql', 'algorithm', 'api', 'backend', 'frontend',
        'bug', 'syntax', 'compile', 'execute', 'script', 'package',
        'import', 'export', 'variable', 'loop', 'conditional', 'refactor',
        'optimize code', 'write code', 'fix code', 'review code',
        'implementation', 'coding', 'developer', 'program'
    ]
    
    # File processing keywords
    file_keywords = ['file', 'document', 'upload', 'large file', 'csv', 'json', 'xml', 'yaml']
    
    # PDF keywords
    pdf_keywords = ['pdf', 'document analysis', 'extract text', 'read pdf']
    
    # Image/photo keywords
    image_keywords = ['image', 'photo', 'picture', 'jpeg', 'png', 'analyze image', 'vision']
    
    # Video keywords
    video_keywords = ['video', 'mp4', 'avi', 'analyze video', 'video processing']
    
    # Check for coding content
    if any(keyword in prompt_lower for keyword in coding_keywords):
        return 'code'
    
    # Check for PDF content
    if any(keyword in prompt_lower for keyword in pdf_keywords):
        return 'pdf'
    
    # Check for image content
    if any(keyword in prompt_lower for keyword in image_keywords):
        return 'image'
    
    # Check for video content
    if any(keyword in prompt_lower for keyword in video_keywords):
        return 'video'
    
    # Check for file content
    if any(keyword in prompt_lower for keyword in file_keywords):
        return 'file'
    
    # Check for code blocks or patterns
    if '```' in prompt or re.search(r'def |class |function |import |const |var |let ', prompt):
        return 'code'
    
    return 'general'


def select_model_for_content(prompt, requested_model=None):
    """Select appropriate model based on content type.
    
    Args:
        prompt: User prompt/message
        requested_model: User-requested model (optional)
    
    Returns:
        str: Model name to use
    """
    from flask import current_app
    
    # If user specifically requested a model, use it
    if requested_model and requested_model in MODELS:
        return requested_model
    
    content_type = detect_content_type(prompt)
    
    # Route to appropriate model based on content type
    if content_type == 'code':
        return 'deepseek'
    elif content_type in ['pdf', 'file']:
        return 'llama'
    elif content_type in ['image', 'video']:
        return 'vicuna'
    else:
        try:
            default_model = current_app.config.get('DEFAULT_MODEL', 'gpt4all')
            return default_model if default_model in MODELS else 'gpt4all'
        except RuntimeError:
            return 'gpt4all'


def get_model_response(prompt, model_name='auto', user=None):
    """Get response from specified model.
    
    Args:
        prompt: User prompt/message
        model_name: Model to use ('auto' for automatic selection)
        user: User object
    
    Returns:
        str: AI response
    """
    from flask import current_app
    
    # Auto-select model based on content if requested
    if model_name == 'auto':
        model_name = select_model_for_content(prompt)
    elif model_name not in MODELS:
        try:
            model_name = current_app.config.get('DEFAULT_MODEL', 'gpt4all')
            if model_name not in MODELS:
                model_name = 'gpt4all'
        except RuntimeError:
            model_name = 'gpt4all'
    
    model = MODELS[model_name]
    
    # Generate response
    response = model.generate(prompt, user)
    
    # Add model info for transparency
    model_status = "loaded" if model.is_loaded() else "mock"
    content_type = detect_content_type(prompt)
    
    if not model.is_loaded():
        response += f"\\n\\n[Using mock response - actual {model_name} model not loaded]"
    
    return response


def get_available_models():
    """Get list of available models with their status."""
    models_info = [
        {
            'id': 'auto',
            'name': 'Auto-Select',
            'description': 'Automatically selects the best model for your task',
            'recommended': True,
            'loaded': True
        },
        {
            'id': 'deepseek',
            'name': 'DeepSeek Coder',
            'description': 'Specialized for coding, debugging, and programming tasks',
            'use_case': 'Coding & Development',
            'loaded': MODELS['deepseek'].is_loaded()
        },
        {
            'id': 'gpt4all',
            'name': 'GPT4All',
            'description': 'General purpose conversational AI for everyday tasks',
            'use_case': 'General Chat',
            'loaded': MODELS['gpt4all'].is_loaded()
        },
        {
            'id': 'llama',
            'name': 'Llama.cpp',
            'description': 'Optimized for document processing and large files',
            'use_case': 'Files & Documents',
            'loaded': MODELS['llama'].is_loaded()
        },
        {
            'id': 'vicuna',
            'name': 'Vicuna',
            'description': 'Multimodal model for images, videos, and rich content',
            'use_case': 'Images & Videos',
            'loaded': MODELS['vicuna'].is_loaded()
        }
    ]
    return models_info
'''
    
    # Write updated content
    try:
        with open(service_file, 'w') as f:
            f.write(updated_service)
        print("   ‚úÖ model_service.py updated successfully")
        return True
    except Exception as e:
        print(f"   ‚ùå Failed to update model_service.py: {str(e)}")
        return False


def auto_integrate_models(use_lite=False):
    """Automatically integrate downloaded models."""
    print("\n" + "=" * 70)
    print("üîß Automatic Model Integration")
    print("=" * 70)
    
    # Step 1: Check for models
    print("\n1Ô∏è‚É£  Checking for downloaded models...")
    available_models = check_models_exist(use_lite)
    
    if not available_models:
        print("   ‚ùå No models found. Run download_models.py first.")
        return False
    
    print(f"   ‚úÖ Found {len(available_models)} model(s)")
    for model_key, model_path in available_models.items():
        file_size = Path(model_path).stat().st_size / (1024**3)
        print(f"      ‚Ä¢ {model_key}: {file_size:.2f} GB")
    
    # Step 2: Install llama-cpp-python
    print("\n2Ô∏è‚É£  Installing dependencies...")
    if not install_llama_cpp_python():
        print("   ‚ö†Ô∏è  Failed to install llama-cpp-python")
        print("   Models downloaded but not integrated")
        print("   Try manual installation: pip install llama-cpp-python")
        return False
    
    # Step 3: Update model service
    print("\n3Ô∏è‚É£  Integrating models into platform...")
    if not update_model_service(available_models):
        print("   ‚ö†Ô∏è  Failed to update model service")
        return False
    
    # Step 4: Update requirements.txt
    print("\n4Ô∏è‚É£  Updating requirements.txt...")
    req_file = Path('./requirements.txt')
    if req_file.exists():
        with open(req_file, 'r') as f:
            reqs = f.read()
        
        # Check if llama-cpp-python exists (with or without version)
        if not any(line.strip().startswith('llama-cpp-python') for line in reqs.split('\n')):
            with open(req_file, 'a') as f:
                f.write('\nllama-cpp-python==0.2.20\n')
            print("   ‚úÖ Added llama-cpp-python to requirements.txt")
        else:
            print("   ‚ÑπÔ∏è  llama-cpp-python already in requirements.txt")
    
    # Success!
    print("\n" + "=" * 70)
    print("‚úÖ INTEGRATION COMPLETE!")
    print("=" * 70)
    print("\nüéâ Models are now integrated and ready to use!")
    print("\nüìä Status:")
    print("   ‚úÖ Models downloaded")
    print("   ‚úÖ Dependencies installed")
    print("   ‚úÖ Platform configured")
    print("\nüöÄ Next steps:")
    print("   1. Restart the application:")
    print("      docker-compose restart web")
    print("      # or")
    print("      python run.py")
    print("\n   2. Models will automatically load on startup")
    print("\n   3. Start chatting with real AI models!")
    print("\nüí° Tip: Use 'Auto-Select' model for best results")
    print()
    
    return True


def main():
    """Main integration workflow."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Integrate downloaded models with the platform',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
This script checks for downloaded models and integrates them automatically.

Usage modes:
  --auto        Automatically integrate models (install deps, update code)
  --check       Just check status without making changes
  --lite        Use lite model variants

Auto-integration will:
1. Check for downloaded models
2. Install llama-cpp-python with appropriate flags
3. Update model_service.py to load actual models
4. Update requirements.txt

After integration, restart the app and models will work automatically!
        """
    )
    
    parser.add_argument(
        '--lite',
        action='store_true',
        help='Check for lite model variants'
    )
    
    parser.add_argument(
        '--generate-env',
        action='store_true',
        help='Generate environment configuration for .env file'
    )
    
    parser.add_argument(
        '--auto',
        action='store_true',
        help='Automatically integrate models (recommended)'
    )
    
    parser.add_argument(
        '--check',
        action='store_true',
        help='Only check status without making changes'
    )
    
    args = parser.parse_args()
    
    # Auto integration mode
    if args.auto:
        success = auto_integrate_models(args.lite)
        sys.exit(0 if success else 1)
    
    # Check mode (original behavior)
    print("=" * 70)
    print("üîß Model Integration Status")
    print("=" * 70)
    print()
    
    # Check for available models
    available_models = check_models_exist(args.lite)
    
    if not available_models:
        print("‚ùå No models found in ./models directory")
        print("\nüì• Run this command to download models:")
        print("   python download_models.py")
        if args.lite:
            print("   python download_models.py --lite")
        print()
        print("üí° TIP: Use --auto flag for automatic integration:")
        print("   python download_models.py  # Downloads and auto-integrates")
        return
    
    print(f"‚úÖ Found {len(available_models)} model(s):")
    for model_key, model_path in available_models.items():
        file_size = Path(model_path).stat().st_size / (1024**3)  # GB
        print(f"   ‚Ä¢ {model_key}: {model_path} ({file_size:.2f} GB)")
    print()
    
    # Current status
    print("üìä Current Status:")
    print("   ‚Ä¢ Models are downloaded and ready")
    print("   ‚Ä¢ Platform is using MOCK adapters (placeholder responses)")
    print("   ‚Ä¢ To use real models, follow integration steps below")
    print()
    
    # Integration steps
    print("üîß Integration Steps:")
    print()
    print("1. Install llama-cpp-python:")
    print("   pip install llama-cpp-python")
    print()
    print("   For GPU acceleration (NVIDIA CUDA):")
    print("   CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python")
    print()
    print("   For Apple Silicon (M1/M2/M3/M4) - Metal acceleration:")
    print("   CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python")
    print()
    
    print("2. Update app/services/model_service.py:")
    print("   See integration example in the auto-generated comment section")
    print()
    
    print("3. Restart the application:")
    print("   docker-compose restart web")
    print("   # or")
    print("   python run.py")
    print()
    
    # Generate environment config
    if args.generate_env:
        env_config = create_env_config(available_models)
        print("üìù Environment Configuration:")
        print(env_config)
        print()
        print("Add the above to your .env file")
        print()
    
    # Show integration code example
    print("üí° Integration Code Example:")
    print("-" * 70)
    integration_code = generate_model_service_integration(available_models)
    print(integration_code)
    print("-" * 70)
    print()
    
    print("‚úÖ Integration check complete!")
    print()
    print("üìö For more details, see docs/MODELS.md")


if __name__ == '__main__':
    main()
