"""Model service for AI interactions."""
from abc import ABC, abstractmethod
import os


class ModelAdapter(ABC):
    """Base class for model adapters."""
    
    @abstractmethod
    def generate(self, prompt, user=None):
        """Generate a response from the model."""
        pass
    
    @abstractmethod
    def get_name(self):
        """Get model name."""
        pass


class LlamaCppAdapter(ModelAdapter):
    """Adapter for llama.cpp models."""
    
    def __init__(self, model_path=None):
        self.model = None
        self.model_path = model_path or os.path.join('models', 'llama-2-7b.Q4_K_M.gguf')
        
        # Try to initialize llama-cpp-python
        try:
            from llama_cpp import Llama
            if os.path.exists(self.model_path):
                self.model = Llama(
                    model_path=self.model_path,
                    n_ctx=2048,
                    n_threads=4,
                    n_gpu_layers=0  # Set to > 0 if you have GPU
                )
                print(f"✓ Llama model loaded from {self.model_path}")
            else:
                print(f"⚠ Model file not found: {self.model_path}")
        except ImportError:
            print("⚠ llama-cpp-python not installed. Install with: pip install llama-cpp-python")
        except Exception as e:
            print(f"⚠ Error loading Llama model: {e}")
    
    def generate(self, prompt, user=None):
        """Generate response using llama.cpp."""
        if self.model:
            try:
                output = self.model(
                    prompt,
                    max_tokens=512,
                    temperature=0.7,
                    top_p=0.95,
                    echo=False,
                    stop=["User:", "Human:"]
                )
                return output['choices'][0]['text'].strip()
            except Exception as e:
                return f"Error generating response: {str(e)}"
        else:
            return "Model not loaded. Please ensure the model file exists in the models directory."
    
    def get_name(self):
        return "llama.cpp"


class GPT4AllAdapter(ModelAdapter):
    """Adapter for GPT4All models."""
    
    def __init__(self, model_path=None):
        self.model = None
        self.model_path = model_path or os.path.join('models', 'gpt4all-falcon-newbpe-q4_0.gguf')
        
        # Try to initialize GPT4All
        try:
            from gpt4all import GPT4All
            if os.path.exists(self.model_path):
                self.model = GPT4All(self.model_path)
                print(f"✓ GPT4All model loaded from {self.model_path}")
            else:
                print(f"⚠ Model file not found: {self.model_path}")
        except ImportError:
            print("⚠ gpt4all not installed. Install with: pip install gpt4all")
        except Exception as e:
            print(f"⚠ Error loading GPT4All model: {e}")
    
    def generate(self, prompt, user=None):
        """Generate response using GPT4All."""
        if self.model:
            try:
                # Clean prompt from context markers
                clean_prompt = prompt.split("Assistant:")[-1].strip() if "Assistant:" in prompt else prompt
                
                response = self.model.generate(
                    clean_prompt,
                    max_tokens=512,
                    temp=0.7,
                    top_p=0.95
                )
                return response.strip()
            except Exception as e:
                return f"Error generating response: {str(e)}"
        else:
            return "Model not loaded. Please ensure the model file exists in the models directory."
    
    
    def get_name(self):
        return "gpt4all"


class DeepSeekAdapter(ModelAdapter):
    """Adapter for DeepSeek Coder models."""
    
    def __init__(self, model_path=None):
        self.model = None
        self.model_path = model_path or os.path.join('models', 'deepseek-coder-6.7b-instruct.Q4_K_M.gguf')
        
        # Try to initialize DeepSeek with llama-cpp-python
        try:
            from llama_cpp import Llama
            if os.path.exists(self.model_path):
                self.model = Llama(
                    model_path=self.model_path,
                    n_ctx=2048,
                    n_threads=4,
                    n_gpu_layers=0
                )
                print(f"✓ DeepSeek Coder model loaded from {self.model_path}")
            else:
                print(f"⚠ Model file not found: {self.model_path}")
        except ImportError:
            print("⚠ llama-cpp-python not installed. Install with: pip install llama-cpp-python")
        except Exception as e:
            print(f"⚠ Error loading DeepSeek model: {e}")
    
    def generate(self, prompt, user=None):
        """Generate response using DeepSeek - specialized for coding."""
        if self.model:
            try:
                # Format for DeepSeek Coder
                formatted_prompt = f"### Instruction:\n{prompt}\n\n### Response:\n"
                output = self.model(
                    formatted_prompt,
                    max_tokens=512,
                    temperature=0.3,  # Lower temp for code generation
                    top_p=0.95,
                    echo=False,
                    stop=["###", "User:", "Instruction:"]
                )
                return output['choices'][0]['text'].strip()
            except Exception as e:
                return f"Error generating code: {str(e)}"
        else:
            return "DeepSeek Coder model not loaded. Please ensure the model file exists."
    
    def get_name(self):
        return "deepseek"


class VicunaAdapter(ModelAdapter):
    """Adapter for Vicuna models."""
    
    def __init__(self, model_path=None):
        self.model = None
        self.model_path = model_path or os.path.join('models', 'vicuna-7b-v1.5.Q4_K_M.gguf')
        
        # Try to initialize Vicuna with llama-cpp-python
        try:
            from llama_cpp import Llama
            if os.path.exists(self.model_path):
                self.model = Llama(
                    model_path=self.model_path,
                    n_ctx=2048,
                    n_threads=4,
                    n_gpu_layers=0
                )
                print(f"✓ Vicuna model loaded from {self.model_path}")
            else:
                print(f"⚠ Model file not found: {self.model_path}")
        except ImportError:
            print("⚠ llama-cpp-python not installed")
        except Exception as e:
            print(f"⚠ Error loading Vicuna model: {e}")
    
    def generate(self, prompt, user=None):
        """Generate response using Vicuna."""
        if self.model:
            try:
                output = self.model(
                    prompt,
                    max_tokens=512,
                    temperature=0.7,
                    top_p=0.95,
                    echo=False,
                    stop=["USER:", "ASSISTANT:"]
                )
                return output['choices'][0]['text'].strip()
            except Exception as e:
                return f"Error generating response: {str(e)}"
        else:
            return "Vicuna model not loaded. Please ensure the model file exists."
    
    def get_name(self):
        return "vicuna"


# Model registry
MODELS = {
    'llama': LlamaCppAdapter(),
    'gpt4all': GPT4AllAdapter(),
    'deepseek': DeepSeekAdapter(),
    'vicuna': VicunaAdapter()
}


def detect_content_type(prompt):
    """Detect content type from prompt to select appropriate model.
    
    Returns:
        str: Content type - 'code', 'file', 'pdf', 'image', 'video', 'general'
    """
    prompt_lower = prompt.lower()
    
    # Coding keywords
    coding_keywords = [
        'code', 'function', 'class', 'programming', 'debug', 'error',
        'python', 'javascript', 'java', 'c++', 'rust', 'go', 'php',
        'html', 'css', 'sql', 'algorithm', 'api', 'backend', 'frontend',
        'bug', 'syntax', 'compile', 'execute', 'script', 'package',
        'import', 'export', 'variable', 'loop', 'conditional', 'refactor',
        'optimize code', 'write code', 'fix code', 'review code',
        'implementation', 'coding', 'developer', 'program'
    ]
    
    # File processing keywords
    file_keywords = ['file', 'document', 'upload', 'large file', 'csv', 'json', 'xml', 'yaml']
    
    # PDF keywords
    pdf_keywords = ['pdf', 'document analysis', 'extract text', 'read pdf']
    
    # Image/photo keywords
    image_keywords = ['image', 'photo', 'picture', 'jpeg', 'png', 'analyze image', 'vision']
    
    # Video keywords
    video_keywords = ['video', 'mp4', 'avi', 'analyze video', 'video processing']
    
    # Check for coding content
    if any(keyword in prompt_lower for keyword in coding_keywords):
        return 'code'
    
    # Check for PDF content
    if any(keyword in prompt_lower for keyword in pdf_keywords):
        return 'pdf'
    
    # Check for image content
    if any(keyword in prompt_lower for keyword in image_keywords):
        return 'image'
    
    # Check for video content
    if any(keyword in prompt_lower for keyword in video_keywords):
        return 'video'
    
    # Check for file content
    if any(keyword in prompt_lower for keyword in file_keywords):
        return 'file'
    
    # Check for code blocks or patterns
    if '```' in prompt or re.search(r'def |class |function |import |const |var |let ', prompt):
        return 'code'
    
    return 'general'


def select_model_for_content(prompt, requested_model=None):
    """Select appropriate model based on content type.
    
    Args:
        prompt: User prompt/message
        requested_model: User-requested model (optional)
    
    Returns:
        str: Model name to use
    """
    from flask import current_app
    
    # If user specifically requested a model, use it
    if requested_model and requested_model in MODELS:
        return requested_model
    
    content_type = detect_content_type(prompt)
    
    # Route to appropriate model based on content type
    if content_type == 'code':
        # Use DeepSeek for coding tasks
        return 'deepseek'
    elif content_type in ['pdf', 'file']:
        # Use Llama for file/document processing
        return 'llama'
    elif content_type in ['image', 'video']:
        # Use Vicuna for multimodal content
        return 'vicuna'
    else:
        # Use configured default model for general chat
        try:
            default_model = current_app.config.get('DEFAULT_MODEL', 'gpt4all')
            return default_model if default_model in MODELS else 'gpt4all'
        except RuntimeError:
            # Outside app context, use gpt4all
            return 'gpt4all'


def get_model_response(prompt, model_name='auto', user=None, history=None):
    """Get response from specified model with conversation context.
    
    Args:
        prompt: User prompt/message
        model_name: Model to use ('auto' for automatic selection)
        user: User object
        history: List of previous messages for context (optional)
    
    Returns:
        str: AI response
    """
    from flask import current_app
    
    # Auto-select model based on content if requested
    if model_name == 'auto':
        model_name = select_model_for_content(prompt)
    elif model_name not in MODELS:
        # Fallback to configured default model if invalid model specified
        try:
            model_name = current_app.config.get('DEFAULT_MODEL', 'gpt4all')
            if model_name not in MODELS:
                model_name = 'gpt4all'
        except RuntimeError:
            model_name = 'gpt4all'
    
    model = MODELS[model_name]
    
    # Build context-aware prompt if history provided (internal use only)
    if history and len(history) > 0:
        # Build context string from conversation history
        context_parts = []
        for msg in history[-10:]:  # Last 10 messages for context
            role = "User" if msg['role'] == 'user' else "Assistant"
            context_parts.append(f"{role}: {msg['content'][:200]}")  # Limit each message to 200 chars
        
        context_parts.append(f"User: {prompt}")
        context_parts.append("Assistant:")
        
        full_prompt = "\n".join(context_parts)
    else:
        full_prompt = prompt
    
    # Generate response (context is hidden from user)
    response = model.generate(full_prompt, user)
    
    return response


def get_available_models():
    """Get list of available models."""
    return [
        {
            'id': 'auto',
            'name': 'Auto-Select',
            'description': 'Automatically selects the best model for your task',
            'recommended': True
        },
        {
            'id': 'deepseek',
            'name': 'DeepSeek Coder',
            'description': 'Specialized for coding, debugging, and programming tasks',
            'use_case': 'Coding & Development'
        },
        {
            'id': 'gpt4all',
            'name': 'GPT4All',
            'description': 'General purpose conversational AI for everyday tasks',
            'use_case': 'General Chat'
        },
        {
            'id': 'llama',
            'name': 'Llama.cpp',
            'description': 'Optimized for document processing and large files',
            'use_case': 'Files & Documents'
        },
        {
            'id': 'vicuna',
            'name': 'Vicuna',
            'description': 'Multimodal model for images, videos, and rich content',
            'use_case': 'Images & Videos'
        }
    ]
